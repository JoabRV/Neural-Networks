# -*- coding: utf-8 -*-
"""Neural Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qUh4eQdvOw6FMQ5dYCAsHj5hFnOXaxpM
"""

import numpy as np

# Paso 1: Definir la estructura de la red
input_size = 2
hidden_size = 4
output_size = 1

# Paso 2: Inicialización de parámetros
np.random.seed(42)  # Para reproducibilidad
weights_input_hidden = np.random.rand(input_size, hidden_size)
bias_hidden = np.random.rand(hidden_size)
weights_hidden_output = np.random.rand(hidden_size, output_size)
bias_output = np.random.rand(output_size)

# Paso 3: Definir funciones de activación
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Paso 4: Propagación hacia adelante
def forward_propagation(inputs):
    hidden_sum = np.dot(inputs, weights_input_hidden) + bias_hidden
    hidden_activation = sigmoid(hidden_sum)
    output_sum = np.dot(hidden_activation, weights_hidden_output) + bias_output
    output = sigmoid(output_sum)
    return output, hidden_activation  # Devolvemos hidden_activation también

# Paso 5: Función de pérdida
def loss_function(predictions, targets):
    return np.mean((predictions - targets) ** 2)

# Datos de entrenamiento (inputs y targets)
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
targets = np.array([[0], [1], [1], [0]])

# Hiperparámetros
learning_rate = 0.1
epochs = 10000

# Entrenamiento
for epoch in range(epochs):
    # Propagación hacia adelante
    predictions, hidden_activation = forward_propagation(inputs)

    # Calcular el error
    loss = loss_function(predictions, targets)

    # Paso 6: Retropropagación
    error = predictions - targets
    d_output = error * predictions * (1 - predictions)
    d_hidden = np.dot(d_output, weights_hidden_output.T) * hidden_activation * (1 - hidden_activation)

    # Actualizar pesos y sesgos
    weights_hidden_output -= learning_rate * np.dot(hidden_activation.T, d_output)
    bias_output -= learning_rate * np.sum(d_output, axis=0)
    weights_input_hidden -= learning_rate * np.dot(inputs.T, d_hidden)
    bias_hidden -= learning_rate * np.sum(d_hidden, axis=0)

    if epoch % 1000 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# Prueba con nuevos datos
new_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
predictions, _ = forward_propagation(new_data)
print("Predictions:", predictions)

import numpy as np

# Paso 1: Definir la estructura de la red
tamaño_entrada = 2
tamaño_oculta = 4
tamaño_salida = 1

# Paso 2: Inicialización de parámetros
np.random.seed(42)  # Para reproducibilidad
pesos_entrada_oculta = np.random.rand(tamaño_entrada, tamaño_oculta)
sesgo_oculta = np.random.rand(tamaño_oculta)
pesos_oculta_salida = np.random.rand(tamaño_oculta, tamaño_salida)
sesgo_salida = np.random.rand(tamaño_salida)

# Paso 3: Definir funciones de activación
def sigmoidal(x):
    return 1 / (1 + np.exp(-x))

# Paso 4: Propagación hacia adelante
def propagacion_hacia_adelante(entradas):
    suma_oculta = np.dot(entradas, pesos_entrada_oculta) + sesgo_oculta
    activacion_oculta = sigmoidal(suma_oculta)
    suma_salida = np.dot(activacion_oculta, pesos_oculta_salida) + sesgo_salida
    salida = sigmoidal(suma_salida)
    return salida, activacion_oculta  # Devolvemos activacion_oculta también

# Paso 5: Función de pérdida
def funcion_de_perdida(predicciones, objetivos):
    return np.mean((predicciones - objetivos) ** 2)

# Datos de entrenamiento (entradas y objetivos)
entradas = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
objetivos = np.array([[0], [1], [1], [0]])

# Hiperparámetros
tasa_aprendizaje = 0.1
epocas = 10000

# Entrenamiento
for epoca in range(epocas):
    # Propagación hacia adelante
    predicciones, activacion_oculta = propagacion_hacia_adelante(entradas)

    # Calcular el error
    perdida = funcion_de_perdida(predicciones, objetivos)

    # Paso 6: Retropropagación
    error = predicciones - objetivos
    d_salida = error * predicciones * (1 - predicciones)
    d_oculta = np.dot(d_salida, pesos_oculta_salida.T) * activacion_oculta * (1 - activacion_oculta)

    # Actualizar pesos y sesgos
    pesos_oculta_salida -= tasa_aprendizaje * np.dot(activacion_oculta.T, d_salida)
    sesgo_salida -= tasa_aprendizaje * np.sum(d_salida, axis=0)
    pesos_entrada_oculta -= tasa_aprendizaje * np.dot(entradas.T, d_oculta)
    sesgo_oculta -= tasa_aprendizaje * np.sum(d_oculta, axis=0)

    if epoca % 1000 == 0:
        print(f"Época {epoca}, Pérdida: {perdida:.4f}")

# Prueba con nuevos datos
nuevos_datos = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
predicciones, _ = propagacion_hacia_adelante(nuevos_datos)
print("Predicciones:", predicciones)